{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a5b1eb2-04ec-4a25-b5db-866c6719c29f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 23068\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "import spacy\n",
    "from random import randrange, shuffle, random, randint\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --------------------------------------------\n",
    "# Data Preparation\n",
    "# --------------------------------------------\n",
    "# Load BookCorpus dataset from Hugging Face and take a 100k-sample subset\n",
    "dataset = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "subset_100k = dataset.select(range(100000))\n",
    "texts = subset_100k['text']\n",
    "\n",
    "# Save texts to a file\n",
    "with open(\"bookcorpus_subset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in texts:\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "# Read raw text and process with spaCy\n",
    "with open(\"bookcorpus_subset.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def process_text_in_chunks(raw_text, chunk_size=1000000):\n",
    "    chunks = [raw_text[i:i+chunk_size] for i in range(0, len(raw_text), chunk_size)]\n",
    "    all_sentences = []\n",
    "    for chunk in chunks:\n",
    "        doc = nlp(chunk)\n",
    "        sentences = list(doc.sents)\n",
    "        all_sentences.extend(sentences)\n",
    "    return all_sentences\n",
    "\n",
    "sentences = process_text_in_chunks(raw_text)\n",
    "sampled_sentences = sentences[:100000]  # limit to 100k sentences\n",
    "\n",
    "# Clean the sentences\n",
    "text = [re.sub(r\"[.,!?\\\\-]\", '', sentence.text.lower()) for sentence in sampled_sentences]\n",
    "\n",
    "# Build vocabulary\n",
    "word_list = list(set(\" \".join(text).split()))\n",
    "word2id = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "for i, word in enumerate(word_list):\n",
    "    word2id[word] = i + 4\n",
    "id2word = {i: word for i, word in enumerate(word2id)}\n",
    "vocab_size = len(word2id)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693c592e-66ad-4aeb-86ac-88b648246e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences (convert words to ids)\n",
    "token_list = []\n",
    "for sentence in text:\n",
    "    token_list.append([word2id[word] for word in sentence.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55964e68-b4b1-4858-85c4-2c14ae719e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1000]) torch.Size([6, 1000]) torch.Size([6, 5]) torch.Size([6, 5]) torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Batch generation function\n",
    "# --------------------------------------------\n",
    "batch_size = 6\n",
    "max_mask = 5\n",
    "max_len = 1000\n",
    "\n",
    "def make_batch(batch_size=6, max_mask=5, max_len=1000):\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        candidates_masked_pos = [i for i, token in enumerate(input_ids) if token not in (word2id['[CLS]'], word2id['[SEP]'])]\n",
    "        shuffle(candidates_masked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in candidates_masked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            r = random()\n",
    "            if r < 0.1:  # random token\n",
    "                index = randint(0, vocab_size - 1)\n",
    "                input_ids[pos] = word2id[id2word[index]]\n",
    "            elif r < 0.9:  # replace with [MASK]\n",
    "                input_ids[pos] = word2id['[MASK]']\n",
    "            else:\n",
    "                pass\n",
    "        n_pad = max_len - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "        if max_mask > n_pred:\n",
    "            n_pad = max_mask - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "        # For NSP: use a simple rule (here positive if consecutive indices)\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
    "            negative += 1\n",
    "    return batch\n",
    "\n",
    "batch = make_batch(batch_size, max_mask, max_len)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "print(input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext.shape)\n",
    "assert input_ids.shape == torch.Size([6, max_len])\n",
    "assert segment_ids.shape == torch.Size([6, max_len])\n",
    "assert masked_tokens.shape == torch.Size([6, max_mask])\n",
    "assert masked_pos.shape == torch.Size([6, max_mask])\n",
    "assert isNext.shape == torch.Size([6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a15783-9dd4-4ab1-90c3-7178bc28971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Model components\n",
    "# --------------------------------------------\n",
    "n_layers = 6\n",
    "n_heads  = 8\n",
    "d_model  = 768\n",
    "d_ff = d_model * 4\n",
    "d_k = d_v = 64\n",
    "n_segments = 2\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)       # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)    # segment embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand_as(x)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    len_k = seq_k.size(1)\n",
    "    pad_attn_mask = seq_k.eq(0).unsqueeze(1)  # 1 for PAD tokens\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "        self.linear = nn.Linear(n_heads * d_v, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        residual = Q\n",
    "        batch_size = Q.size(0)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)\n",
    "        output = self.linear(context)\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output, attn\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d36520-0f9f-465c-947f-6a78356a9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# BERT Model Definition\n",
    "# --------------------------------------------\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        # For NSP and MLM\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # Share weights for MLM: tie decoder to token embedding weights\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        self.decoder = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(vocab_size))\n",
    "    def forward(self, input_ids, segment_ids, masked_pos, attention_mask=None):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        if attention_mask is not None:\n",
    "            enc_self_attn_mask = enc_self_attn_mask * attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        for layer in self.layers:\n",
    "            output, _ = layer(output, enc_self_attn_mask)\n",
    "        # Next Sentence Prediction (use [CLS] token representation)\n",
    "        h_pooled = self.activ(self.fc(output[:, 0]))  # [batch_size, d_model]\n",
    "        logits_nsp = self.classifier(h_pooled)         # [batch_size, 2]\n",
    "        # Masked Language Modeling: gather output for masked positions\n",
    "        masked_pos_expanded = masked_pos[:, :, None].expand(-1, -1, output.size(-1))\n",
    "        h_masked = torch.gather(output, 1, masked_pos_expanded)\n",
    "        h_masked = self.norm(F.gelu(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias\n",
    "        return logits_lm, logits_nsp\n",
    "\n",
    "    def get_sentence_embedding(self, input_ids, segment_ids):\n",
    "        \"\"\"\n",
    "        Returns the pooled [CLS] representation that can be used as the sentence embedding.\n",
    "        \"\"\"\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output, _ = layer(output, enc_self_attn_mask)\n",
    "        h_pooled = self.activ(self.fc(output[:, 0]))\n",
    "        return h_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724cf363-8eae-47a5-81d9-e4d3d78fabbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch: 000 loss = 107.271065\n",
      "Best model saved at epoch 0 with loss 107.271065\n",
      "Best model saved at epoch 9 with loss 103.135223\n",
      "Best model saved at epoch 10 with loss 101.163689\n",
      "Best model saved at epoch 11 with loss 91.795013\n",
      "Best model saved at epoch 13 with loss 83.215134\n",
      "Best model saved at epoch 15 with loss 72.218376\n",
      "Best model saved at epoch 18 with loss 67.562126\n",
      "Best model saved at epoch 19 with loss 56.529549\n",
      "Best model saved at epoch 26 with loss 53.223518\n",
      "Best model saved at epoch 28 with loss 42.277283\n",
      "Best model saved at epoch 36 with loss 42.113510\n",
      "Best model saved at epoch 38 with loss 41.771416\n",
      "Best model saved at epoch 40 with loss 39.090973\n",
      "Best model saved at epoch 41 with loss 35.485992\n",
      "Best model saved at epoch 52 with loss 31.839205\n",
      "Best model saved at epoch 59 with loss 31.565279\n",
      "Best model saved at epoch 60 with loss 30.824751\n",
      "Best model saved at epoch 61 with loss 29.999926\n",
      "Best model saved at epoch 72 with loss 29.092245\n",
      "Best model saved at epoch 73 with loss 23.003654\n",
      "Best model saved at epoch 79 with loss 19.138117\n",
      "Best model saved at epoch 85 with loss 15.587865\n",
      "Best model saved at epoch 91 with loss 11.630436\n",
      "Epoch: 100 loss = 15.992108\n",
      "Best model saved at epoch 125 with loss 11.153173\n",
      "Best model saved at epoch 144 with loss 7.961955\n",
      "Epoch: 200 loss = 16.647449\n",
      "Best model saved at epoch 264 with loss 7.126428\n",
      "Best model saved at epoch 286 with loss 5.387479\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Training Loop\n",
    "# --------------------------------------------\n",
    "model = BERT()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 300\n",
    "best_loss = float('inf')\n",
    "start_time = torch.cuda.Event(enable_timing=True)\n",
    "end_time = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    batch = make_batch(batch_size, max_mask, max_len)\n",
    "    input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens)\n",
    "    loss_lm = loss_lm.mean()\n",
    "    loss_nsp = criterion(logits_nsp, isNext)\n",
    "    loss = loss_lm + loss_nsp\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch:03d} loss = {loss.item():.6f}')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), 'bert_model_for_sbert.pth')\n",
    "        print(f'Best model saved at epoch {epoch} with loss {best_loss:.6f}')\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dbbc706-729c-49b1-8d41-68d6d48775db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted sentence embedding shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example: Using the saved model to extract sentence embeddings\n",
    "# (Task 2 can now load this state_dict into an identical BERT instance and call get_sentence_embedding)\n",
    "loaded_model = BERT()\n",
    "loaded_model.load_state_dict(torch.load('bert_model_for_sbert.pth'))\n",
    "loaded_model.eval()\n",
    "# Example dummy input (using first sentence from token_list)\n",
    "dummy_input_ids = torch.LongTensor([ [word2id['[CLS]']] + token_list[0][:max_len-2] + [word2id['[SEP]']] + [0]*(max_len - (len(token_list[0]) + 2)) ])\n",
    "dummy_segment_ids = torch.zeros_like(dummy_input_ids)\n",
    "sentence_embedding = loaded_model.get_sentence_embedding(dummy_input_ids, dummy_segment_ids)\n",
    "print(\"Extracted sentence embedding shape:\", sentence_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d41fbc40-2020-4e41-af33-838306217658",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba8604ca-b1d0-4a0a-b2dd-e030c1f57641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (embedding): Embedding(\n",
       "    (tok_embed): Embedding(23068, 768)\n",
       "    (pos_embed): Embedding(1000, 768)\n",
       "    (seg_embed): Embedding(2, 768)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (enc_self_attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (linear): Linear(in_features=512, out_features=768, bias=True)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_ffn): PoswiseFeedForwardNet(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activ): Tanh()\n",
       "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (decoder): Linear(in_features=768, out_features=23068, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained BERT model\n",
    "model = BERT()  # Your BERT class from Task 1\n",
    "model.load_state_dict(torch.load('bert_model_for_sbert.pth'))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "957d1089-4628-4ef9-93d7-123abc10f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SNLI dataset\n",
    "dataset = load_dataset(\"snli\")\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "622d8382-0e6e-4a49-a397-cbe868bedc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_snli(data):\n",
    "    sentences1 = []\n",
    "    sentences2 = []\n",
    "    labels = []\n",
    "    for example in data:\n",
    "        if example[\"label\"] != -1:  # Filter out invalid examples\n",
    "            sentences1.append(example[\"premise\"])\n",
    "            sentences2.append(example[\"hypothesis\"])\n",
    "            labels.append(example[\"label\"])\n",
    "    return sentences1, sentences2, labels\n",
    "\n",
    "train_sentences1, train_sentences2, train_labels = preprocess_snli(train_data)\n",
    "val_sentences1, val_sentences2, val_labels = preprocess_snli(val_data)\n",
    "test_sentences1, test_sentences2, test_labels = preprocess_snli(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a37fa60-368f-4db5-b22d-5785f7162550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences, word2id, max_len=100):\n",
    "    tokenized = []\n",
    "    for sentence in sentences:\n",
    "        tokens = [word2id.get(word, word2id[\"[PAD]\"]) for word in sentence.split()]\n",
    "        tokens = tokens[:max_len] + [word2id[\"[PAD]\"]] * (max_len - len(tokens))  # Pad to max_len\n",
    "        tokenized.append(tokens)\n",
    "    return torch.LongTensor(tokenized)\n",
    "\n",
    "max_len = 100  # Maximum sequence length\n",
    "train_tokens1 = tokenize_sentences(train_sentences1, word2id, max_len)\n",
    "train_tokens2 = tokenize_sentences(train_sentences2, word2id, max_len)\n",
    "val_tokens1 = tokenize_sentences(val_sentences1, word2id, max_len)\n",
    "val_tokens2 = tokenize_sentences(val_sentences2, word2id, max_len)\n",
    "test_tokens1 = tokenize_sentences(test_sentences1, word2id, max_len)\n",
    "test_tokens2 = tokenize_sentences(test_sentences2, word2id, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f724a24b-3506-45ae-afcd-5ccc19c6f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBERT(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(SentenceBERT, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.classifier = nn.Linear(d_model * 3, 3)  # 3 classes for SNLI (entailment, neutral, contradiction)\n",
    "\n",
    "    def forward(self, input_ids1, segment_ids1, input_ids2, segment_ids2):\n",
    "        # Get sentence embeddings for both sentences\n",
    "        u = self.bert.get_sentence_embedding(input_ids1, segment_ids1)  # [batch_size, d_model]\n",
    "        v = self.bert.get_sentence_embedding(input_ids2, segment_ids2)  # [batch_size, d_model]\n",
    "\n",
    "        # Compute the element-wise absolute difference\n",
    "        diff = torch.abs(u - v)  # [batch_size, d_model]\n",
    "\n",
    "        # Concatenate u, v, and |u - v|\n",
    "        combined = torch.cat([u, v, diff], dim=1)  # [batch_size, d_model * 3]\n",
    "\n",
    "        # Pass through the classifier\n",
    "        logits = self.classifier(combined)  # [batch_size, 3]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ae30c-84a0-4164-94fc-a5baae23990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Sentence-BERT model\n",
    "sbert_model = SentenceBERT(model).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(sbert_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "best_val_loss = float('inf')  # Track the best validation loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    sbert_model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Training phase\n",
    "    for i in range(0, len(train_tokens1), batch_size):\n",
    "        # Get a batch of data\n",
    "        input_ids1 = train_tokens1[i:i+batch_size].to(device)\n",
    "        input_ids2 = train_tokens2[i:i+batch_size].to(device)\n",
    "        labels = torch.LongTensor(train_labels[i:i+batch_size]).to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = sbert_model(input_ids1, torch.zeros_like(input_ids1), input_ids2, torch.zeros_like(input_ids2))\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    sbert_model.eval()\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(val_tokens1), batch_size):\n",
    "            input_ids1 = val_tokens1[i:i+batch_size].to(device)\n",
    "            input_ids2 = val_tokens2[i:i+batch_size].to(device)\n",
    "            labels = torch.LongTensor(val_labels[i:i+batch_size]).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = sbert_model(input_ids1, torch.zeros_like(input_ids1), input_ids2, torch.zeros_like(input_ids2))\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Accumulate validation loss\n",
    "            val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    # Calculate average validation loss for the epoch\n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(sbert_model.state_dict(), 'best_sbert_model.pth')\n",
    "        print(f\"Best model saved with Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e309eb-1909-4326-af98-09d087adb68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have completed the training for above task and saved the model but accidently i ran this again and the ouput dissapeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4fefac-474f-4b4f-b737-1f8ae93080fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84f50a39-4cb5-456c-bb76-ef8ad0cca1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceBERT(\n",
       "  (bert): BERT(\n",
       "    (embedding): Embedding(\n",
       "      (tok_embed): Embedding(23068, 768)\n",
       "      (pos_embed): Embedding(1000, 768)\n",
       "      (seg_embed): Embedding(2, 768)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (enc_self_attn): MultiHeadAttention(\n",
       "          (W_Q): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (W_K): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (W_V): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (linear): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ffn): PoswiseFeedForwardNet(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activ): Tanh()\n",
       "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (decoder): Linear(in_features=768, out_features=23068, bias=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=2304, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best Sentence-BERT model\n",
    "sbert_model = SentenceBERT(model).to(device)\n",
    "sbert_model.load_state_dict(torch.load('best_sbert_model.pth'))\n",
    "sbert_model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3c1bd26-38cb-40c1-9bc3-a9bdad66e2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.00      0.00      0.00      3368\n",
      "      neutral       0.00      0.00      0.00      3219\n",
      "contradiction       0.33      1.00      0.50      3237\n",
      "\n",
      "     accuracy                           0.33      9824\n",
      "    macro avg       0.11      0.33      0.17      9824\n",
      " weighted avg       0.11      0.33      0.16      9824\n",
      "\n",
      "Accuracy: 0.3295\n",
      "Precision: 0.1086\n",
      "Recall: 0.3295\n",
      "F1-Score: 0.1633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125405/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125405/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125405/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st125405/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(test_tokens1, test_tokens2, test_labels, batch_size=8):\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_tokens1), batch_size):\n",
    "            # Get a batch of data\n",
    "            input_ids1 = test_tokens1[i:i+batch_size].to(device)\n",
    "            input_ids2 = test_tokens2[i:i+batch_size].to(device)\n",
    "            labels = torch.LongTensor(test_labels[i:i+batch_size]).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = sbert_model(input_ids1, torch.zeros_like(input_ids1), input_ids2, torch.zeros_like(input_ids2))\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(true_labels, predictions, target_names=['entailment', 'neutral', 'contradiction']))\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Evaluate on the test set\n",
    "accuracy, precision, recall, f1 = evaluate_model(test_tokens1, test_tokens2, test_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d2bbeb-134c-418b-ad70-b218cb2acdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The classification report shows that the model struggles with classifying \"entailment\" and \"neutral\" labels, \n",
    "#as both have zero precision, recall, and F1 scores, indicating poor performance on these classes. It performs well on \n",
    "#\"contradiction\" with a perfect recall (1.00) but low precision (0.33), resulting in a low F1 score of 0.50. Overall, \n",
    "#the model's accuracy is 32.95%, with poor performance on the minority classes, as seen in the macro and weighted averages. \n",
    "#This suggests the need for improvements, such as addressing class imbalance and model optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca5b675-8eec-401b-89b2-7a9999c418a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK 3 PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29df120-7c12-4df8-b161-91292ebc4627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Sentence-BERT model for NLI tasks faces challenges such as class imbalance, limited dataset size, and high computational costs.\n",
    "#Performance can also be sensitive to hyperparameters, and overfitting is a risk with small datasets. To improve, techniques like class balancing, \n",
    "#using larger datasets, leveraging pre-trained models, and hyperparameter tuning can enhance results. Regularization methods, advanced architectures\n",
    "#like RoBERTa, and data augmentation can help with generalization, while using additional metrics like MCC can better assess performance. \n",
    "#Utilizing cloud-based GPUs and conducting error analysis will optimize training and refine the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
